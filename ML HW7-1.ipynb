{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test accuracy rate by data points from 6601 to 10000__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  # y labels are oh-encoded\n",
    "n_train = mnist.train.num_examples  # 55,000\n",
    "n_validation = mnist.validation.num_examples  # 5000\n",
    "n_test = mnist.test.num_examples  # 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784  # input layer (28x28 pixels)\n",
    "n_hidden1 = 512  # 1st hidden layer\n",
    "n_hidden2 = 256  # 2nd hidden layer\n",
    "n_hidden3 = 128  # 3rd hidden layer\n",
    "n_output = 10  # output layer (0-9 digits)\n",
    "learning_rate = 1e-4\n",
    "n_iterations = 1000\n",
    "batch_size = 128\n",
    "dropout = 0.5\n",
    "cut_start = 6601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1': tf.Variable(tf.truncated_normal([n_input, n_hidden1], stddev=0.1)),\n",
    "    'w2': tf.Variable(tf.truncated_normal([n_hidden1, n_hidden2], stddev=0.1)),\n",
    "    'w3': tf.Variable(tf.truncated_normal([n_hidden2, n_hidden3], stddev=0.1)),\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden3, n_output], stddev=0.1)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.constant(0.1, shape=[n_hidden1])),\n",
    "    'b2': tf.Variable(tf.constant(0.1, shape=[n_hidden2])),\n",
    "    'b3': tf.Variable(tf.constant(0.1, shape=[n_hidden3])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_output]))\n",
    "}\n",
    "\n",
    "layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "layer_drop = tf.nn.dropout(layer_3, keep_prob)\n",
    "output_layer = tf.matmul(layer_drop, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=Y, logits=output_layer\n",
    "        ))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_drop_list = []\n",
    "iteration = 1\n",
    "while iteration <= 5:\n",
    "    for i in range(n_iterations):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step, feed_dict={\n",
    "            X: batch_x, Y: batch_y, keep_prob: dropout\n",
    "            })\n",
    "\n",
    "        # print loss and accuracy (per minibatch)\n",
    "        if i % 100 == 0:\n",
    "            minibatch_loss, minibatch_accuracy = sess.run(\n",
    "                [cross_entropy, accuracy],\n",
    "                feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0}\n",
    "                )\n",
    "#             print(\n",
    "#                 \"Iteration\",\n",
    "#                 str(i),\n",
    "#                 \"\\t| Loss =\",\n",
    "#                 str(minibatch_loss),\n",
    "#                 \"\\t| Accuracy =\",\n",
    "#                 str(minibatch_accuracy)\n",
    "#                 )\n",
    "    X_test = mnist.test.images[cut_start:]\n",
    "    Y_test = mnist.test.labels[cut_start:]\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: X_test, Y: Y_test, keep_prob: 1.0})\n",
    "    accuracy_drop_list.append(test_accuracy)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94463074"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracy_drop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Re-do the entire exercise but without using the dropouts from the final layer. Calculate accuracy of your new model and compare against your previously achieved accuracy. What is your conclusion? Please also explain the intuition behind dropouts.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = tf.add(tf.matmul(X, weights['w1']), biases['b1'])\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "layer_3 = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "output_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=Y, logits=output_layer\n",
    "        ))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(output_layer, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "iteration = 1\n",
    "while iteration <= 5:\n",
    "    for i in range(n_iterations):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step, feed_dict={\n",
    "            X: batch_x, Y: batch_y\n",
    "            })\n",
    "\n",
    "        # print loss and accuracy (per minibatch)\n",
    "        if i % 100 == 0:\n",
    "            minibatch_loss, minibatch_accuracy = sess.run(\n",
    "                [cross_entropy, accuracy],\n",
    "                feed_dict={X: batch_x, Y: batch_y}\n",
    "                )\n",
    "#             print(\n",
    "#                 \"Iteration\",\n",
    "#                 str(i),\n",
    "#                 \"\\t| Loss =\",\n",
    "#                 str(minibatch_loss),\n",
    "#                 \"\\t| Accuracy =\",\n",
    "#                 str(minibatch_accuracy)\n",
    "#                 )\n",
    "    X_test = mnist.test.images[cut_start:]\n",
    "    Y_test = mnist.test.labels[cut_start:]\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: X_test, Y: Y_test})\n",
    "    accuracy_list.append(test_accuracy)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9449838"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What other conventional algorithm(s) could you have used for this task? What was the advantage of using a neural network for this exercise, instead of any other potential conventional algorithms?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since y variable is categorical range from 0 to 9, we can use multinomial logistic regression to identify the number. However, as 28 * 28 pixels, x variable is highly dimensional, with which MNL is hard to handle. The advantage of Neural network to be applied in this exercise is that three layers are utilized to figure out a better combination of weights that in a way reduces dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use a multinomial logistic regression model to achieve the same handwriting identification task and compare the out-of-sample accuracy against the results of the neural network.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images\n",
    "Y_train_array = mnist.train.labels\n",
    "Y_train = np.array([list(array).index(1.) for array in Y_train_array])\n",
    "clf = LogisticRegression(random_state=0, multi_class='multinomial', solver='newton-cg')\n",
    "model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923310093967869"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_cat = np.array([list(array).index(1.) for array in Y_test])\n",
    "model.score(X_test, Y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
